{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connexion à la première page\n",
    "url = \"https://www.paruvendu.fr/immobilier/annonceimmofo/liste/listeAnnonces?tt=1&at=1&nbp0=99&pa=FR&lo=75,77,78,91,92,93,94,95&lol=0&ray=50\"\n",
    "\n",
    "# Url ancien\n",
    "url = \"https://www.paruvendu.fr/immobilier/annonceimmofo/liste/listeAnnonces?nbp=0&tt=1&tbApp=1&tbDup=1&tbChb=1&tbLof=1&tbAtl=1&tbPla=1&tbMai=1&tbVil=1&tbCha=1&tbPro=1&tbHot=1&tbMou=1&tbFer=1&at=-4&nbp0=99&pa=FR&lo=75,77,78,91,92,93,94,95&ddlFiltres=nofilter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_list = [[] for x in range(6)]\n",
    "agence_list =[]\n",
    "def normalize_text(text):\n",
    "    temp = str(text).replace(\"\\t\",\"\")\n",
    "    temp = str(temp).replace(\"\\n\",\"\")\n",
    "    temp = str(temp).replace(\"\\r\",\" \")\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, scrap_list): \n",
    "    root_url =\"https://www.paruvendu.fr\"\n",
    "    r = requests.get(url)\n",
    "    if r.connection :\n",
    "        soup = BeautifulSoup(r.text,\"lxml\")\n",
    "        \n",
    "        h3 = soup.find_all(\"div\", {\"class\":\"ergov3-h3\"})\n",
    "        for elem in h3:     \n",
    "            scrap_list[0].append(normalize_text(elem.span.text))\n",
    "            scrap_list[1].append(normalize_text(elem.cite.text))\n",
    "        \n",
    "        priceannonce = soup.find_all(\"div\", {\"class\":\"ergov3-priceannonce\"})\n",
    "        for elem in priceannonce: \n",
    "            scrap_list[2].append(normalize_text(elem.text))\n",
    "        \n",
    "        textannonce = soup.find_all(\"p\", {\"class\":\"txt-long\"})\n",
    "        for elem in textannonce: \n",
    "            scrap_list[3].append(normalize_text(elem.text))\n",
    "        \n",
    "\n",
    "        lienpro = soup.find_all(\"div\", {\"class\":\"enslogoname_2lines\"})\n",
    "        for elem in lienpro: \n",
    "            scrap_list[4].append(normalize_text(elem.span.text))\n",
    "\n",
    "        agencepro = soup.find_all(\"div\", {\"class\":\"ergov3-lienpro_2lines\"})\n",
    "        for elem in agencepro: \n",
    "            try :\n",
    "                agence_list.append(normalize_text(elem.span.text))\n",
    "            except AttributeError : \n",
    "                agence_list.append(\"PARTICULIER\")\n",
    "        \n",
    "        link_url = soup.find_all(\"a\", {\"class\":\"voirann\"})\n",
    "        for elem in link_url:\n",
    "            if ((str(elem[\"href\"]).startswith(\"#\")) or (str(elem[\"href\"]).startswith(\"j\"))):\n",
    "                continue\n",
    "            else :\n",
    "                temp_url = root_url+str(elem[\"href\"])\n",
    "                scrap_list[5].append(temp_url)\n",
    "\n",
    "        \n",
    "    return(scrap_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkerd\\AppData\\Local\\Temp\\ipykernel_26780\\176678893.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Firefox(executable_path=r'geckodriver.exe')\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text,\"lxml\")\n",
    "url = \"https://www.paruvendu.fr/immobilier/annonceimmofo/liste/listeAnnonces?nbp=0&tt=1&tbApp=1&tbDup=1&tbChb=1&tbLof=1&tbAtl=1&tbPla=1&tbMai=1&tbVil=1&tbCha=1&tbPro=1&tbHot=1&tbMou=1&tbFer=1&at=-4&nbp0=99&pa=FR&lo=75,77,78,91,92,93,94,95&ddlFiltres=nofilter\"\n",
    "driver = webdriver.Firefox(executable_path=r'geckodriver.exe')\n",
    "driver.get(url)\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH,'/html/body/div[1]/div/div[1]/div[3]/button[2]'))).click()\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"batchsdk-ui-alert__buttons_negative\"]'))).click()\n",
    "\n",
    "for i in range(100): \n",
    "    sleep(0.1)\n",
    "    end_url=\"&p=\"+str(i+1)\n",
    "    if i+1 == 1 :\n",
    "        scrap_list =get_data(url, scrap_list)\n",
    "    else :\n",
    "        new_url = url+end_url\n",
    "        scrap_list =get_data(new_url, scrap_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "scrap_df = pd.DataFrame(columns= [\"titre_annonce\",\"ville_cp\",\"prix\",\"texte_annonce\",\"type_vendeur\",\"url\"])\n",
    "for i,v in enumerate(scrap_df.columns):\n",
    "    scrap_df[v] = scrap_list[i]\n",
    "\n",
    "scrap_df = scrap_df[~scrap_df[\"titre_annonce\"].str.contains(\"Vente\")]\n",
    "scrap_df[\"nom_agence\"] = agence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scrap(df): \n",
    "    for column in df.columns :\n",
    "        df[column] = df[column].str.strip()\n",
    "    df[\"ville_cp\"] = df[\"ville_cp\"].str.replace(\"([A-Za-z])(\\s)([A-Za-z])\", r\"\\1-\\3\", regex=True)\n",
    "    df[\"code_postal\"] = df['ville_cp'].str.split(\" \").str[1]\n",
    "    df['ville'] = df['ville_cp'].str.split(\" \").str[0] \n",
    "    df[\"code_postal\"] = df[\"code_postal\"].str.replace(\"[()]\",\"\", regex=True)\n",
    "    df['type'] = df['titre_annonce'].str.split(\",\").str[0]\n",
    "    df['surface'] = df['titre_annonce'].str.split(\",\").str[1]\n",
    "    df['nb_pieces'] = df['type'].str.extract('(\\d+)')\n",
    "    df['type'] = df['type'].str.split('(\\d+)', regex = True).str[0]\n",
    "    df['surface'] = df['surface'].str.split(' ').str[0]\n",
    "    df[\"code_postal\"].loc[df[\"ville\"] == \"Paris\"] = df[\"code_postal\"].loc[df[\"ville\"] == \"Paris\"].apply(lambda x: \"75\"+\"0\"*(3-len(x))+str(x))\n",
    "    df['departement'] = df[\"code_postal\"].str[:2]\n",
    "    df[\"prix\"] = df[\"prix\"].str.replace(\"[*€\\s]\",\"\", regex=True)\n",
    "    df.dropna(inplace= True)\n",
    "    df.drop(df[df.code_postal == \"\"].index, inplace=True)\n",
    "    df.reset_index(drop =True, inplace=True)\n",
    "    df[\"id\"] = df.index\n",
    "    df= df[[\"id\",\"type\",\"nb_pieces\",\"surface\",\"prix\",\"ville\",\"code_postal\",\"departement\",\"type_vendeur\",\"texte_annonce\",\"url\",\"nom_agence\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cast_type(df): \n",
    "    for elem in [\"nb_pieces\",\"surface\",\"prix\",\"code_postal\",\"departement\"] :\n",
    "        df[elem] = df[elem].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_df = format_scrap(scrap_df)\n",
    "scrap_df = cast_type(scrap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2494 entries, 0 to 2493\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2494 non-null   int64 \n",
      " 1   type           2494 non-null   object\n",
      " 2   nb_pieces      2494 non-null   int32 \n",
      " 3   surface        2494 non-null   int32 \n",
      " 4   prix           2494 non-null   int32 \n",
      " 5   ville          2494 non-null   object\n",
      " 6   code_postal    2494 non-null   int32 \n",
      " 7   departement    2494 non-null   int32 \n",
      " 8   type_vendeur   2494 non-null   object\n",
      " 9   texte_annonce  2494 non-null   object\n",
      " 10  url            2494 non-null   object\n",
      " 11  nom_agence     2494 non-null   object\n",
      "dtypes: int32(5), int64(1), object(6)\n",
      "memory usage: 185.2+ KB\n"
     ]
    }
   ],
   "source": [
    "scrap_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff8e42e0991f17a1919b04a8858326e2d69de7398ad071dab783899f633a62a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
